#!/usr/bin/env python3
"""Create a very brief PDF memo summarizing scraping status + sample results.

- Reads a website sentiment CSV (WASP or non-WASP)
- Embeds existing plots from exports/plots if present
- Writes a 1–3 page PDF to exports/

Usage:
  python3 scripts/make_scraping_memo_pdf.py \
    --csv wasp_website_sentiment_20251218-082648.csv

If --csv is omitted, it picks the newest wasp_website_sentiment_*.csv in repo root.
"""

from __future__ import annotations

import argparse
from pathlib import Path
from datetime import datetime

import pandas as pd
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages
import matplotlib.image as mpimg


def newest(glob_pat: str) -> Path | None:
    files = sorted(Path('.').glob(glob_pat), reverse=True)
    return files[0] if files else None


def add_text_page(pdf: PdfPages, title: str, lines: list[str], footer: str | None = None) -> None:
    fig = plt.figure(figsize=(8.27, 11.69))  # A4 portrait
    fig.patch.set_facecolor('white')

    y = 0.94
    fig.text(0.08, y, title, fontsize=18, fontweight='bold', va='top')
    y -= 0.05

    for ln in lines:
        fig.text(0.08, y, ln, fontsize=11, va='top')
        y -= 0.03
        if y < 0.12:
            break

    if footer:
        fig.text(0.08, 0.06, footer, fontsize=9, alpha=0.75)

    pdf.savefig(fig)
    plt.close(fig)


def add_image_page(pdf: PdfPages, img_path: Path, title: str | None = None) -> None:
    if not img_path.exists():
        return
    img = mpimg.imread(str(img_path))
    fig = plt.figure(figsize=(11.69, 8.27))  # A4 landscape
    fig.patch.set_facecolor('white')
    ax = fig.add_subplot(111)
    ax.imshow(img)
    ax.axis('off')
    if title:
        fig.suptitle(title, fontsize=14)
    fig.tight_layout()
    pdf.savefig(fig)
    plt.close(fig)


def main() -> int:
    ap = argparse.ArgumentParser()
    ap.add_argument('--csv', default='')
    ap.add_argument('--plots-dir', default='exports/plots')
    ap.add_argument('--out', default='')
    args = ap.parse_args()

    csv_path = Path(args.csv) if args.csv else (newest('wasp_website_sentiment_*.csv') or newest('exports/wasp_website_sentiment_*.csv'))
    if not csv_path or not csv_path.exists():
        raise SystemExit('Could not find input CSV. Pass --csv <path>.')

    df = pd.read_csv(csv_path)
    df['text_len'] = pd.to_numeric(df.get('text_len', 0), errors='coerce').fillna(0).astype(int)

    n = len(df)
    n_ok = int((df['text_len'] > 0).sum())
    n_zero = int((df['text_len'] == 0).sum())

    # Mini results table (top 8 by text_len)
    show_cols = [c for c in ['company_name', 'num_pages_total', 'num_pages_aboutish', 'text_len', 'polarity', 'subjectivity'] if c in df.columns]
    top = df.sort_values('text_len', ascending=False)[show_cols].head(8)

    now = datetime.now()
    ts = now.strftime('%Y%m%d-%H%M%S')

    out_path = Path(args.out) if args.out else Path('exports') / f'scraping_memo_{ts}.pdf'
    out_path.parent.mkdir(parents=True, exist_ok=True)

    plots_dir = Path(args.plots_dir)

    # pick newest plots if present
    def newest_plot(prefix: str) -> Path | None:
        files = sorted(plots_dir.glob(f'{prefix}_*.png'), reverse=True)
        return files[0] if files else None

    p_cov = newest_plot('coverage_text_len')
    p_dist = newest_plot('normalized_distributions')
    p_feat = newest_plot('normalized_features_by_company')

    title = 'Scraping memo — website sentiment (WASP pilot)'
    lines = [
        f'Date: {now.strftime('%Y-%m-%d %H:%M')}',
        '',
        'Goal:',
        '  Collect "About / Mission / Sustainability" website text for shipping companies and compute simple sentiment features',
        '  to compare WASP adopters vs non‑WASP control group for regression / similarity matching.',
        '',
        'What is implemented (current state):',
        '  - CompanyProfilerV3 crawls key pages on official company sites (prefers Wikidata; can also use Gemini when quota works).',
        '  - Exporters compute sentiment using TextBlob (polarity + subjectivity) from scraped website text.',
        '  - Non‑WASP pipeline exists and runs in batches (not executed yet in this local workspace).',
        '',
        f'Current CSV: {csv_path}',
        f'Coverage: {n} companies total; {n_ok} with usable website text; {n_zero} with 0 text (needs better website resolution / retries).',
        '',
        'Columns:',
        '  - text_len: total characters of scraped text used for sentiment (0 means no usable website text)',
        '  - polarity: [-1..+1] negative→positive tone',
        '  - subjectivity: [0..1] factual→opinionated/marketing tone',
        '',
        'Sample results (top by text_len):',
        top.to_string(index=False),
    ]

    footer = 'Generated by scripts/make_scraping_memo_pdf.py'

    with PdfPages(str(out_path)) as pdf:
        add_text_page(pdf, title, lines, footer=footer)
        if p_cov:
            add_image_page(pdf, p_cov, title='Scrape coverage (text_len per company)')
        if p_dist:
            add_image_page(pdf, p_dist, title='Normalized distributions (log1p/z-score)')
        if p_feat:
            add_image_page(pdf, p_feat, title='Per-company normalized features (z-scores)')

    print('Wrote:', out_path)
    return 0


if __name__ == '__main__':
    raise SystemExit(main())
